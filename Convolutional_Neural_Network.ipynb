{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional_Neural_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Q2nAOEZwWsot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y10ZMOBGXHk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IEIwuJrZXRWf",
        "colab_type": "code",
        "outputId": "c06e97ca-4255-4107-d825-472cd1119c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "mnist=input_data.read_data_sets(\"/temp/data/\",one_hot=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-16d542ecb6a4>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /temp/data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /temp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /temp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /temp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V1UJQldhXUaE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes=10\n",
        "batch_size=128\n",
        "x=tf.placeholder('float')\n",
        "y=tf.placeholder('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TThlWVlZXfr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x,W):\n",
        "  \n",
        "    #this function is for convolution layer\n",
        "    \n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
        "  \n",
        "  \n",
        "  \n",
        "def polling(x):\n",
        "  \n",
        "    #this function is for pooling from convolution layer we use max pooling here\n",
        "    \n",
        "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ybhef5wdXmSw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolutional_neural_network(x):\n",
        "    weights={'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
        "            'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
        "            'W_fc1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
        "            'out':tf.Variable(tf.random_normal([1024,n_classes]))}\n",
        "    \n",
        "    \n",
        "    \n",
        "    biases={'b_conv1':tf.Variable(tf.random_normal([32])),\n",
        "            'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "            'b_fc1':tf.Variable(tf.random_normal([1024])),\n",
        "            'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "    \n",
        "    \n",
        "    x=tf.reshape(x,shape=[-1,28,28,1])\n",
        "    \n",
        "    \n",
        "    #First convolution layer\n",
        "    conv1=conv2d(x,weights['W_conv1'])\n",
        "    \n",
        "    #first pooling layer\n",
        "    conv1=polling(conv1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Second convolution layer\n",
        "    conv2=conv2d(conv1,weights['W_conv2'])\n",
        "    \n",
        "    #Second pooling layer\n",
        "    conv2=polling(conv2)\n",
        "    \n",
        "    '''The convolution layer followed by a pooling form one layer of CNN. \n",
        "    We have created to layer of convolution followed by pooling .Now I'm\n",
        "    going to create a fully connected layer '''\n",
        "    \n",
        "    \n",
        "    fc=tf.reshape(conv2,[-1,7*7*64])\n",
        "    \n",
        "    fc1=tf.nn.relu(tf.matmul(fc,weights['W_fc1']+biases['b_fc1']))\n",
        "    \n",
        "    output=tf.matmul(fc1,weights['out'])+biases['out']\n",
        "   \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-WwV8NPXnFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(x):\n",
        "  \n",
        "    prediction=convolutional_neural_network(x)\n",
        "    \n",
        "    #softmax_cross_entropy_with_logits_v2 is used to to calculate the cross entropy between logits and labels\n",
        "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
        "    \n",
        "    #we use here adam optimizer for backpropagation to reduce the loss\n",
        "    optimizer=tf.train.AdamOptimizer().minimize(cost) \n",
        "     \n",
        "    \n",
        "    hm_epoch=10\n",
        "    with tf.Session() as ses:\n",
        "      \n",
        "        ses.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for epoch in range(hm_epoch):\n",
        "          \n",
        "            epoch_loss=0\n",
        "            \n",
        "            for _ in range(int(mnist.train.num_examples/(batch_size))):\n",
        "              \n",
        "                a,b=mnist.train.next_batch(batch_size)\n",
        "                #here 'a' contains feature of next batch \n",
        "                #And 'b' contain labels of corresponding Row\n",
        "                \n",
        "                k,c=ses.run([optimizer,cost],feed_dict={x:a,y:b})\n",
        "                #Start running network and data feeding here\n",
        "                \n",
        "                epoch_loss+=c\n",
        "                \n",
        "            print(\"epoch\",epoch+1,\"completed out of\",hm_epoch,\"loss=\",epoch_loss)\n",
        "            \n",
        "        correct=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
        "        \n",
        "        accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
        "        print(\"accuracy:\",accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LwDxmlpIX7du",
        "colab_type": "code",
        "outputId": "5fe10be9-8c0c-417d-b038-e68356945251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "train_neural_network(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "epoch 1 completed out of 10 loss= 40273066.66461182\n",
            "epoch 2 completed out of 10 loss= 806048.0044555664\n",
            "epoch 3 completed out of 10 loss= 498678.7469177246\n",
            "epoch 4 completed out of 10 loss= 375293.73944091797\n",
            "epoch 5 completed out of 10 loss= 270201.52962493896\n",
            "epoch 6 completed out of 10 loss= 208937.0088043213\n",
            "epoch 7 completed out of 10 loss= 164991.4130859375\n",
            "epoch 8 completed out of 10 loss= 137877.69285583496\n",
            "epoch 9 completed out of 10 loss= 116814.76477050781\n",
            "epoch 10 completed out of 10 loss= 90102.00406646729\n",
            "accuracy: 0.9687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J0yEVb0LdX_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}